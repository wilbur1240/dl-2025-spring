{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9f70ab2",
      "metadata": {
        "id": "e9f70ab2"
      },
      "source": [
        "# HW2-3 Image captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aada0b51",
      "metadata": {
        "id": "aada0b51"
      },
      "source": [
        "### Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3970b0a",
      "metadata": {
        "id": "f3970b0a"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth pandas pyarrow torch torchvision datasets transformer accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a3cc040",
      "metadata": {
        "id": "2a3cc040"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PIL import PILImage\n",
        "from datasets import Dataset, Features, Image, Value\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from unsloth import FastVisionModel\n",
        "import json\n",
        "\n",
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from transformers import TextStreamer\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "660ab0ce",
      "metadata": {
        "id": "660ab0ce"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db653afd",
      "metadata": {
        "id": "db653afd"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(parquet_file, include_caption=True):\n",
        "    df = pd.read_parquet(parquet_file)\n",
        "    df['image'] = df['image'].apply(lambda x: x['bytes'])  # flatten {'bytes': ...}\n",
        "\n",
        "    features = Features({\n",
        "        'image': Image(decode=True),\n",
        "        'caption': Value('string') if include_caption else Value('null')  # null type if test set\n",
        "    })\n",
        "\n",
        "    columns = ['image', 'caption'] if include_caption else ['image']\n",
        "    dataset = Dataset.from_pandas(df[columns], features=features)\n",
        "\n",
        "    def resize_image(example):\n",
        "        img = example['image']\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = img.resize((224, 224))\n",
        "        return {'image': img}\n",
        "\n",
        "    dataset = dataset.map(resize_image)\n",
        "    return dataset\n",
        "\n",
        "train_ds = prepare_dataset(\"train_data.parquet\", include_caption=True)\n",
        "valid_ds = prepare_dataset(\"valid_data.parquet\", include_caption=True)\n",
        "\n",
        "test_df = pd.read_parquet(\"test_data.parquet\")\n",
        "test_df['image'] = test_df['image'].apply(lambda x: x['bytes'])\n",
        "\n",
        "# Test dataset: build Hugging Face Dataset (with idx)\n",
        "features_test = Features({\n",
        "    'idx': Value('int64'),\n",
        "    'image': Image(decode=True)\n",
        "})\n",
        "test_ds = Dataset.from_pandas(test_df[['idx', 'image']], features=features_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b23d318c",
      "metadata": {
        "id": "b23d318c"
      },
      "source": [
        "### Pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e4e12fb",
      "metadata": {
        "id": "6e4e12fb"
      },
      "outputs": [],
      "source": [
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82102837",
      "metadata": {
        "id": "82102837"
      },
      "source": [
        "### Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af46c80a",
      "metadata": {
        "id": "af46c80a"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting"
      ],
      "metadata": {
        "id": "3x7BElw5KCrM"
      },
      "id": "3x7BElw5KCrM"
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"You are an expert artist. Describe accurately what you see in this image.\"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "    conversation = [\n",
        "        { \"role\": \"user\",\n",
        "          \"content\" : [\n",
        "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
        "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
        "        },\n",
        "        { \"role\" : \"assistant\",\n",
        "          \"content\" : [\n",
        "            {\"type\" : \"text\",  \"text\"  : sample[\"caption\"]} ]\n",
        "        },\n",
        "    ]\n",
        "    return { \"messages\" : conversation }\n",
        "pass\n",
        "\n",
        "converted_dataset = [convert_to_conversation(sample) for sample in train_df]"
      ],
      "metadata": {
        "id": "JFVTfPV7KEit"
      },
      "id": "JFVTfPV7KEit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "LASmN5c3KpnJ"
      },
      "id": "LASmN5c3KpnJ"
    },
    {
      "cell_type": "code",
      "source": [
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
        "    train_dataset = converted_dataset,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 30,\n",
        "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bf16_supported(),\n",
        "        bf16 = is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",     # For Weights and Biases\n",
        "\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc = 4,\n",
        "        max_seq_length = 2048,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "_NmMZYAQKoPR"
      },
      "id": "_NmMZYAQKoPR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "nlG9WldpK3Sw"
      },
      "id": "nlG9WldpK3Sw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving"
      ],
      "metadata": {
        "id": "U7i-xBRkLzGB"
      },
      "id": "U7i-xBRkLzGB"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"Llama-3.2-11B-Vision-caption-finetune-unsloth\") # Local saving\n",
        "tokenizer.save_pretrained(\"Llama-3.2-11B-Vision-caption-finetune-unsloth\")\n",
        "# model.push_to_hub(\"Wilbur1240/Llama-3.2-11B-Vision-caption-finetune-unsloth\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"Wilbur1240/Llama-3.2-11B-Vision-caption-finetune-unsloth\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "YbKP4PGhL01x"
      },
      "id": "YbKP4PGhL01x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b9c55e33",
      "metadata": {
        "id": "b9c55e33"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34fabbc2",
      "metadata": {
        "id": "34fabbc2"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "results = []\n",
        "\n",
        "instruction = \"You are an expert artist. Describe accurately what you see in this image.\"\n",
        "\n",
        "for example in tqdm(test_df):\n",
        "    image = example['image']\n",
        "    idx = example['idx']\n",
        "\n",
        "    # Build the chat prompt\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction}\n",
        "        ]}\n",
        "    ]\n",
        "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate the caption\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        use_cache=True,\n",
        "        temperature=1.5,\n",
        "        min_p=0.1,\n",
        "    )\n",
        "\n",
        "    # Decode\n",
        "    caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Collect result\n",
        "    results.append({\n",
        "        \"idx\": idx,\n",
        "        \"output\": caption.strip()\n",
        "    })\n",
        "\n",
        "# 4️⃣ Save results to JSON (like sample_submission.json)\n",
        "with open('submission.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"✅ Inference complete. Results saved to submission.json.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}