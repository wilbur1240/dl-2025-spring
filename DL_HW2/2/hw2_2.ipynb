{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75bafae6",
   "metadata": {},
   "source": [
    "# HW2-2 Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dd5ff",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0db1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from accelerate import Accelerator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(4096)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed(4096)\n",
    "\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10df81",
   "metadata": {},
   "source": [
    "### Dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bbcacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder,\n",
    "        image_size\n",
    "    ):\n",
    "        self.folder = folder\n",
    "        self.image_size = image_size\n",
    "        self.paths = [p for p in Path(f'{folder}').glob(f'**/*.jpg')]\n",
    "        #################################\n",
    "        ## TODO: Data Augmentation ##\n",
    "        #################################\n",
    "        self.transform = T.Compose([\n",
    "            # T.Resize(image_size),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomResizedCrop(image_size, scale=(0.9, 1.0)),\n",
    "            T.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4b750",
   "metadata": {},
   "source": [
    "### Noise Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3282c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"\n",
    "    linear schedule, proposed in original paper\n",
    "    \"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "# e.g. from Nichol & Dhariwal’s “improved DDPM”:\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return betas.clamp(max=0.999)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74419eff",
   "metadata": {},
   "source": [
    "### U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9919ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization functions\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "# iteration function\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "# small helper modules\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "    \n",
    "def Upsample(dim, dim_out=None):\n",
    "    out_dim = dim_out if dim_out is not None else dim\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        nn.Conv2d(dim, out_dim, 3, padding=1)\n",
    "    )\n",
    "\n",
    "def Downsample(dim, dim_out=None):\n",
    "    out_dim = dim_out if dim_out is not None else dim\n",
    "    return nn.Sequential(\n",
    "        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2),\n",
    "        nn.Conv2d(dim * 4, out_dim, 1)\n",
    "    )\n",
    "\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        w = self.weight\n",
    "\n",
    "        # compute per-out-channel mean & variance over (in_channels, kH, kW)\n",
    "        mean = w.mean(dim=[1,2,3], keepdim=True)                   # shape [out,1,1,1]\n",
    "        var  = w.var(dim= [1,2,3], unbiased=False, keepdim=True)   # shape [out,1,1,1]\n",
    "\n",
    "        # weight-standardize\n",
    "        w_norm = (w - mean) * (var + eps).rsqrt()\n",
    "\n",
    "        return F.conv2d(\n",
    "            x, w_norm, self.bias,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            dilation=self.dilation,\n",
    "            groups=self.groups\n",
    "        )\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        var = torch.var(x, dim=1, unbiased=False, keepdim=True)\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) * (var + eps).rsqrt() * self.g\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "    \n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        return torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "        return self.act(x)\n",
    "    \n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = None\n",
    "        if time_emb_dim is not None:\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_emb_dim, dim_out * 2)\n",
    "            )\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if self.mlp is not None and time_emb is not None:\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "    \n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, dim, 1),\n",
    "            LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads), qkv)\n",
    "        q = q.softmax(dim=-2) * self.scale\n",
    "        k = k.softmax(dim=-1)\n",
    "        v = v / (h * w)\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        resnet_block_groups=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.init_conv = nn.Conv2d(channels, dim, 7, padding=3)\n",
    "        dims = [dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        block_cls = partial(ResnetBlock, groups=resnet_block_groups, time_emb_dim=time_dim)\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                block_cls(dim_in, dim_in),\n",
    "                block_cls(dim_in, dim_in),\n",
    "                block_cls(dim_in, dim_in),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding=1)\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_cls(mid_dim, mid_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, LinearAttention(mid_dim)))\n",
    "        self.mid_block2 = block_cls(mid_dim, mid_dim)\n",
    "        self.mid_block3 = block_cls(mid_dim, mid_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                block_cls(dim_out + dim_in, dim_out),\n",
    "                block_cls(dim_out + dim_in, dim_out),\n",
    "                block_cls(dim_out + dim_in, dim_out),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                Upsample(dim_out, dim_in) if not is_last else nn.Conv2d(dim_out, dim_in, 3, padding=1)\n",
    "            ]))\n",
    "\n",
    "        self.final_res_block = block_cls(dim * 2, dim)\n",
    "        self.final_conv = nn.Conv2d(dim, channels, 1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        x = self.init_conv(x)\n",
    "        residual = x.clone()\n",
    "        t = self.time_mlp(time)\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, block3, attn, down in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "            x = block2(x, t)\n",
    "            h.append(x)\n",
    "            x = block3(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            x = down(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block3(x, t)\n",
    "\n",
    "        for block1, block2, block3, attn, up in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block3(x, t)\n",
    "            x = attn(x)\n",
    "            x = up(x)\n",
    "\n",
    "        x = torch.cat((x, residual), dim=1)\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8543e9",
   "metadata": {},
   "source": [
    "### Diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589cf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        image_size: int, \n",
    "        timesteps: int,\n",
    "        beta_schedule: str = 'linear',\n",
    "        auto_normalize: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.channels = model.channels\n",
    "        self.image_size = image_size\n",
    "        self.num_timesteps = timesteps\n",
    "\n",
    "        # 1) build beta schedule\n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown beta schedule: {beta_schedule}')\n",
    "        \n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "        # 2) register all buffers\n",
    "        self.register_buffer('betas', betas.float())\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod.float())\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev.float())\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod - 1.0).float())\n",
    "\n",
    "        # posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance.float())\n",
    "        self.register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min=1e-20)).float())\n",
    "        self.register_buffer('posterior_mean_coef1', (betas * torch.sqrt(alphas_cumprod_prev) / (1 - alphas_cumprod)).float())\n",
    "        self.register_buffer('posterior_mean_coef2', ((1 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1 - alphas_cumprod)).float())\n",
    "\n",
    "        # 3) Normalization functions\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else (lambda x: x)\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else (lambda x: x)\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        \"\"\"Estimate x_0 from x_t and predicted noise\"\"\"\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t \n",
    "            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "    \n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        \"\"\" Compute mean & variance of q(x_{t-1}) | x_t, x_0 \"\"\"\n",
    "        mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        var = extract(self.posterior_variance, t, x_t.shape)\n",
    "        log_var = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return mean, var, log_var\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, clip_denoised: bool = True):\n",
    "        \"\"\" One diffusion reverse step \"\"\"\n",
    "        b = x.shape[0]\n",
    "        t_batch = torch.full((b,), t, device=x.device, dtype=torch.long)\n",
    "        # model predicts noise\n",
    "        pred_noise = self.model(x, t_batch)\n",
    "        # recover x0; optionally clamp it\n",
    "        x0 = self.predict_start_from_noise(x, t_batch, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x0 = x0.clamp(-1.0, 1.0)\n",
    "\n",
    "        # posterior mean & variance\n",
    "        mean, _, log_var = self.q_posterior(x0, x, t_batch)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0.0\n",
    "        return mean + torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps: bool = False):\n",
    "        \"\"\" Starting from pure noise, run full reverse chain\"\"\"\n",
    "        img = torch.randn(shape, device=self.betas.device)\n",
    "        all_imgs = [img]\n",
    "        for t in tqdm(reversed(range(self.num_timesteps)), desc='sampling'):\n",
    "            img = self.p_sample(img, t)\n",
    "            all_imgs.append(img)\n",
    "        out = torch.stack(all_imgs, dim=1) if return_all_timesteps else img\n",
    "        return self.unnormalize(out)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample_ddim(self, x, t, t_prev, eta=0.0, clip_denoised=True):\n",
    "        \"\"\" One DDIM sampling step from x_t to x_{t_prev}\"\"\"\n",
    "        b = x.shape[0]\n",
    "        t_batch = torch.full((b, ), t, device=x.device, dtype=torch.long)\n",
    "        # predict noise\n",
    "        pred_noise = self.model(x, t_batch)\n",
    "        # pred x0\n",
    "        x0 = self.predict_start_from_noise(x, t_batch, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x0 = x0.clamp(-1, 1)\n",
    "\n",
    "        alpha_t = self.alphas_cumprod[t]\n",
    "        alpha_prev = self.alphas_cumprod[t_prev] if t_prev >= 0 else torch.tensor(1.0, device=x.device)\n",
    "        sqrt_alpha_t = alpha_t.sqrt()\n",
    "        sqrt_alpha_prev = alpha_prev.sqrt()\n",
    "        sigma_t = eta * ((1 - alpha_prev) / (1 - alpha_t) * (1 - alpha_t / alpha_prev)).sqrt()\n",
    "        pred_dir = (1 - alpha_prev - sigma_t ** 2).sqrt() * pred_noise\n",
    "        noise = sigma_t * torch.randn_like(x) if t_prev >= 0 else 0.0\n",
    "        x_prev = sqrt_alpha_prev * x0 + pred_dir + noise\n",
    "\n",
    "        return x_prev, x0\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def ddim_sample_loop(self, shape, num_ddim_steps=50, eta=0.0, return_all_timesteps=False):\n",
    "        \"\"\" Run the fuull DDIM sampling loop \"\"\"\n",
    "        device = self.betas.device\n",
    "        img = torch.randn(shape, device=device)\n",
    "        all_imgs = [img]\n",
    "\n",
    "        # Create a custom timestep schedule\n",
    "        ddim_timesteps = np.linspace(0, self.num_timesteps - 1, num_ddim_steps, dtype=int)\n",
    "\n",
    "        for i in tqdm(range(num_ddim_steps - 1, -1, -1), desc='DDIM sampling'):\n",
    "            t = ddim_timesteps[i]\n",
    "            t_prev = ddim_timesteps[i - 1] if i > 0 else -1\n",
    "\n",
    "            img, _ = self.p_sample_ddim(img, t, t_prev, eta=eta)\n",
    "            all_imgs.append(img)\n",
    "        \n",
    "        out = torch.stack(all_imgs, dim=1) if return_all_timesteps else img\n",
    "        return self.unnormalize(out)\n",
    "\n",
    "    def sample(self, batch_size=16, use_ddim=False, num_ddim_steps=50, eta=0.0, return_all_timesteps=False):\n",
    "        shape = (batch_size, self.channels, self.image_size, self.image_size)\n",
    "        if use_ddim:\n",
    "            return self.ddim_sample_loop(shape, num_ddim_steps=num_ddim_steps, eta=eta, return_all_timesteps=return_all_timesteps)\n",
    "        else:\n",
    "            return self.p_sample_loop(shape, return_all_timesteps=return_all_timesteps)\n",
    "    \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\" Forward noising process \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start + \n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "    \n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        \"\"\" MSE loss between true noise and model's prediction \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        pred_noise = self.model(x_noisy, t)\n",
    "        loss = F.mse_loss(pred_noise, noise, reduction='none')\n",
    "        loss = loss.mean(dim=list(range(1, loss.ndim))) # mean over c, h, w\n",
    "        return loss.mean()\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \"\"\" Training entrypoint: sample random timesteps & return loss \"\"\"\n",
    "        b, c, h, w = img.shape\n",
    "        assert h == self.image_size and w == self.image_size\n",
    "        t = torch.randint(0, self.num_timesteps, (b, ), device=img.device)\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798df8c3",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a30ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model: nn.Module,\n",
    "        data_folder: str,\n",
    "        batch_size: int = 16,\n",
    "        lr: float = 1e-4,\n",
    "        num_steps: int = 100000,\n",
    "        grad_accum_steps: int = 1,\n",
    "        ema_decay: float = 0.995,\n",
    "        save_interval: int = 1000,\n",
    "        num_samples: int = 25,\n",
    "        results_folder: str = './results',\n",
    "        use_ddim = False,\n",
    "        num_ddim_steps=50,\n",
    "        eta = 0.0\n",
    "    ):\n",
    "        # Accelerator\n",
    "        self.accelerator = Accelerator(mixed_precision='no')\n",
    "        self.device = self.accelerator.device\n",
    "\n",
    "        # Training State\n",
    "        self.batch_size       = batch_size\n",
    "        self.grad_accum_steps = grad_accum_steps\n",
    "        self.num_steps        = num_steps\n",
    "        self.save_interval    = save_interval\n",
    "        self.num_samples      = num_samples\n",
    "\n",
    "        # model, optimizer, EMA\n",
    "        self.model = diffusion_model.to(self.device)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=self.num_steps, eta_min=1e-6)\n",
    "        self.model, self.optimizer = self.accelerator.prepare(\n",
    "            self.model, self.optimizer\n",
    "        )\n",
    "        \n",
    "        # Use EMA on the raw model\n",
    "        self.ema = EMA(self.accelerator.unwrap_model(self.model), beta=ema_decay)\n",
    "\n",
    "        # Data\n",
    "        ds = Dataset(data_folder, diffusion_model.image_size)\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "        self.dl = cycle(self.accelerator.prepare(dl))\n",
    "\n",
    "        # checkpoints & samples\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(parents=True, exist_ok=True)\n",
    "        self.step = 0\n",
    "        self.use_ddim = use_ddim\n",
    "        self.num_ddim_steps = num_ddim_steps\n",
    "        self.eta = eta\n",
    "    \n",
    "    def save(self, milestone: int):\n",
    "        \"\"\"Save model, optimizer, EMA and step counter.\"\"\"\n",
    "        if not self.accelerator.is_main_process:\n",
    "            return\n",
    "        ckpt = self.results_folder / f'model-{milestone}.pt'\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'ema':       self.ema.state_dict(),\n",
    "        }\n",
    "        torch.save(data, ckpt)\n",
    "\n",
    "    def load(self, ckpt_path: str):\n",
    "        \"\"\"Load all state (model, optimizer, EMA, step).\"\"\"\n",
    "        data = torch.load(ckpt_path, map_location=self.device)\n",
    "        raw_model = self.accelerator.unwrap_model(self.model)\n",
    "        raw_model.load_state_dict(data['model'])\n",
    "        self.optimizer.load_state_dict(data['optimizer'])\n",
    "        self.ema.load_state_dict(data['ema'])\n",
    "        self.step = data['step']\n",
    "\n",
    "    def _sample_and_save(self, milestone: int):\n",
    "        \"\"\"Generate `num_samples` via EMA model and save grid.\"\"\"\n",
    "        self.ema.ema_model.eval()\n",
    "        batches = num_to_groups(self.num_samples, self.batch_size)\n",
    "        imgs = torch.cat([\n",
    "            self.ema.ema_model.sample(batch_size=n, use_ddim=self.use_ddim, num_ddim_steps=self.num_ddim_steps, eta=self.eta) for n in batches\n",
    "        ], dim=0)\n",
    "        path = self.results_folder / f'sample-{milestone}.png'\n",
    "        vutils.save_image(imgs, path, nrow=int(math.sqrt(self.num_samples)))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Run the training loop with gradient accumulation, EMA updates, and periodic sampling.\"\"\"\n",
    "        pbar = tqdm(total=self.num_steps, initial=self.step, disable=not self.accelerator.is_main_process)\n",
    "        while self.step < self.num_steps:\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # gradient accumulation\n",
    "            for _ in range(self.grad_accum_steps):\n",
    "                batch = next(self.dl).to(self.device)\n",
    "                with self.accelerator.autocast():\n",
    "                    loss = self.model(batch) / self.grad_accum_steps\n",
    "                total_loss += loss.item()\n",
    "                self.accelerator.backward(loss)\n",
    "\n",
    "            # optimizer step\n",
    "            self.accelerator.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.step += 1\n",
    "\n",
    "            # EMA & sampling\n",
    "            if self.accelerator.is_main_process:\n",
    "                self.ema.update()\n",
    "                if self.step % self.save_interval == 0:\n",
    "                    milestone = self.step // self.save_interval\n",
    "                    self._sample_and_save(milestone)\n",
    "                    self.save(milestone)\n",
    "\n",
    "            pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "            pbar.update(1)\n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            print('Training complete.')\n",
    "\n",
    "    def inference(self, total: int = 1000, output_path: str = './submission'):\n",
    "        \"\"\"\n",
    "        Generate `total` images using the same batch size as training\n",
    "        (self.batch_size, e.g. 16), and save them to disk.\n",
    "        \"\"\"\n",
    "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "        count = 0\n",
    "        batch_size = self.batch_size  # manually fixed at 16\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while count < total:\n",
    "                n = min(batch_size, total - count)\n",
    "\n",
    "                # clear any leftover GPU memory\n",
    "                if torch.cuda.is_available():\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # sample n images\n",
    "                imgs = self.ema.ema_model.sample(batch_size=n, use_ddim=self.use_ddim, num_ddim_steps=self.num_ddim_steps, eta=self.eta)\n",
    "\n",
    "                # move to CPU and save\n",
    "                imgs = imgs.cpu()\n",
    "                for img in imgs:\n",
    "                    count += 1\n",
    "                    vutils.save_image(img, f\"{output_path}/{count}.jpg\")\n",
    "\n",
    "                # clean up \n",
    "                del imgs\n",
    "                if torch.cuda.is_available():\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Inference complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e7a7a",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "147e99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e1a6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './face_dataset'\n",
    "IMG_SIZE = 64\n",
    "batch_size = 32\n",
    "train_num_steps = 20000\n",
    "lr = 1e-4\n",
    "grad_steps = 1\n",
    "ema_decay = 0.9999\n",
    "\n",
    "channels = 128\n",
    "dim_mults = (1, 2, 4, 8)\n",
    "\n",
    "timesteps = 1000\n",
    "beta_schedule = 'cosine'\n",
    "\n",
    "use_ddim = True\n",
    "num_ddim_steps = 50\n",
    "eta = 0.0\n",
    "\n",
    "model = Unet(\n",
    "    dim = channels,\n",
    "    dim_mults = dim_mults\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = IMG_SIZE,\n",
    "    timesteps = timesteps,\n",
    "    beta_schedule = beta_schedule\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    data_folder = train_data_path,\n",
    "    batch_size = batch_size,\n",
    "    lr = lr,\n",
    "    num_steps = train_num_steps,\n",
    "    grad_accum_steps = grad_steps,\n",
    "    ema_decay = ema_decay,\n",
    "    save_interval = timesteps,\n",
    "    use_ddim=use_ddim,\n",
    "    num_ddim_steps=num_ddim_steps,\n",
    "    eta=eta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf97ac11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.79 GiB of which 33.44 MiB is free. Process 3795299 has 5.08 GiB memory in use. Of the allocated memory 4.93 GiB is allocated by PyTorch, and 28.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 95\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 95\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_steps\n\u001b[1;32m     96\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 169\u001b[0m, in \u001b[0;36mGaussianDiffusion.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    167\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, (b, ), device\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    168\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(img)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 158\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_losses\u001b[0;34m(self, x_start, t, noise)\u001b[0m\n\u001b[1;32m    156\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x_start)\n\u001b[1;32m    157\u001b[0m x_noisy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sample(x_start, t, noise)\n\u001b[0;32m--> 158\u001b[0m pred_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred_noise, noise, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    160\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, loss\u001b[38;5;241m.\u001b[39mndim))) \u001b[38;5;66;03m# mean over c, h, w\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 223\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, time)\u001b[0m\n\u001b[1;32m    220\u001b[0m h \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block1, block2, block3, attn, down \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdowns:\n\u001b[0;32m--> 223\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     h\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    225\u001b[0m     x \u001b[38;5;241m=\u001b[39m block2(x, t)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 133\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[0;34m(self, x, time_emb)\u001b[0m\n\u001b[1;32m    131\u001b[0m     time_emb \u001b[38;5;241m=\u001b[39m rearrange(time_emb, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c -> b c 1 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m     scale_shift \u001b[38;5;241m=\u001b[39m time_emb\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_shift\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(h)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_conv(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 111\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, scale_shift)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_shift \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     scale, shift \u001b[38;5;241m=\u001b[39m scale_shift\n\u001b[0;32m--> 111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.79 GiB of which 33.44 MiB is free. Process 3795299 has 5.08 GiB memory in use. Of the allocated memory 4.93 GiB is allocated by PyTorch, and 28.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae7de3",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bc0b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling: 1000it [00:18, 53.51it/s]\n",
      "sampling: 1000it [00:18, 53.60it/s]\n",
      "sampling: 1000it [00:18, 53.40it/s]\n",
      "sampling: 1000it [00:18, 53.04it/s]\n",
      "sampling: 1000it [00:18, 53.41it/s]\n",
      "sampling: 1000it [00:18, 52.99it/s]\n",
      "sampling: 1000it [00:18, 53.47it/s]\n",
      "sampling: 1000it [00:18, 52.77it/s]\n",
      "sampling: 1000it [00:18, 53.60it/s]\n",
      "sampling: 1000it [00:18, 53.33it/s]\n",
      "sampling: 1000it [00:18, 53.52it/s]\n",
      "sampling: 1000it [00:18, 53.05it/s]\n",
      "sampling: 1000it [00:18, 53.08it/s]\n",
      "sampling: 1000it [00:18, 53.50it/s]\n",
      "sampling: 1000it [00:18, 53.88it/s]\n",
      "sampling: 1000it [00:18, 53.41it/s]\n",
      "sampling: 1000it [00:18, 53.37it/s]\n",
      "sampling: 1000it [00:18, 52.93it/s]\n",
      "sampling: 1000it [00:18, 53.78it/s]\n",
      "sampling: 1000it [00:18, 53.14it/s]\n",
      "sampling: 1000it [00:19, 52.62it/s]\n",
      "sampling: 1000it [00:18, 53.15it/s]\n",
      "sampling: 1000it [00:19, 52.00it/s]\n",
      "sampling: 1000it [00:18, 52.71it/s]\n",
      "sampling: 1000it [00:19, 52.53it/s]\n",
      "sampling: 1000it [00:18, 53.20it/s]\n",
      "sampling: 1000it [00:18, 53.28it/s]\n",
      "sampling: 1000it [00:18, 53.38it/s]\n",
      "sampling: 1000it [00:18, 54.04it/s]\n",
      "sampling: 1000it [00:18, 53.82it/s]\n",
      "sampling: 1000it [00:18, 53.54it/s]\n",
      "sampling: 1000it [00:18, 53.17it/s]\n",
      "sampling: 1000it [00:18, 53.49it/s]\n",
      "sampling: 1000it [00:18, 53.06it/s]\n",
      "sampling: 1000it [00:18, 53.36it/s]\n",
      "sampling: 1000it [00:18, 52.73it/s]\n",
      "sampling: 1000it [00:18, 53.16it/s]\n",
      "sampling: 1000it [00:18, 53.30it/s]\n",
      "sampling: 1000it [00:18, 53.01it/s]\n",
      "sampling: 1000it [00:18, 53.69it/s]\n",
      "sampling: 1000it [00:18, 53.32it/s]\n",
      "sampling: 1000it [00:19, 52.33it/s]\n",
      "sampling: 1000it [00:19, 52.48it/s]\n",
      "sampling: 1000it [00:18, 53.43it/s]\n",
      "sampling: 1000it [00:18, 53.36it/s]\n",
      "sampling: 1000it [00:18, 53.34it/s]\n",
      "sampling: 1000it [00:19, 52.52it/s]\n",
      "sampling: 1000it [00:18, 53.19it/s]\n",
      "sampling: 1000it [00:18, 52.88it/s]\n",
      "sampling: 1000it [00:18, 53.43it/s]\n",
      "sampling: 1000it [00:18, 53.15it/s]\n",
      "sampling: 1000it [00:18, 53.15it/s]\n",
      "sampling: 1000it [00:18, 53.89it/s]\n",
      "sampling: 1000it [00:18, 53.05it/s]\n",
      "sampling: 1000it [00:18, 53.17it/s]\n",
      "sampling: 1000it [00:18, 53.51it/s]\n",
      "sampling: 1000it [00:18, 53.11it/s]\n",
      "sampling: 1000it [00:18, 52.79it/s]\n",
      "sampling: 1000it [00:18, 53.47it/s]\n",
      "sampling: 1000it [00:18, 53.26it/s]\n",
      "sampling: 1000it [00:18, 53.51it/s]\n",
      "sampling: 1000it [00:18, 53.02it/s]\n",
      "sampling: 1000it [00:14, 68.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete.\n"
     ]
    }
   ],
   "source": [
    "ckpt = './results/model-20.pt'\n",
    "trainer.load(ckpt)\n",
    "trainer.inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
