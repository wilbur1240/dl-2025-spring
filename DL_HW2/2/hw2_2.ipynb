{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75bafae6",
   "metadata": {},
   "source": [
    "# HW2-2 Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dd5ff",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0db1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from accelerate import Accelerator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(4096)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed(4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10df81",
   "metadata": {},
   "source": [
    "### Dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbcacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder,\n",
    "        image_size\n",
    "    ):\n",
    "        self.folder = folder\n",
    "        self.image_size = image_size\n",
    "        self.paths = [p for p in Path(f'{folder}').glob(f'**/*.jpg')]\n",
    "        #################################\n",
    "        ## TODO: Data Augmentation ##\n",
    "        #################################\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(image_size),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path)\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4b750",
   "metadata": {},
   "source": [
    "### Noise Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3282c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"\n",
    "    linear schedule, proposed in original paper\n",
    "    \"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74419eff",
   "metadata": {},
   "source": [
    "### U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9919ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization functions\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "# iteration function\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "# small helper modules\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "    \n",
    "def Upsample(dim, dim_out=None):\n",
    "    out_dim = dim_out if dim_out is not None else dim\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        nn.Conv2d(dim, out_dim, 3, padding=1)\n",
    "    )\n",
    "\n",
    "def Downsample(dim, dim_out=None):\n",
    "    out_dim = dim_out if dim_out is not None else dim\n",
    "    return nn.Sequential(\n",
    "        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2),\n",
    "        nn.Conv2d(dim * 4, out_dim, 1)\n",
    "    )\n",
    "\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, 'o ... -> o 1 1 1', 'mean')\n",
    "        var = reduce(weight, 'o ... -> o 1 1 1', lambda t: torch.var(t, unbiased=False))\n",
    "        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n",
    "        return F.conv2d(x, normalized_weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        var = torch.var(x, dim=1, unbiased=False, keepdim=True)\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) * (var + eps).rsqrt() * self.g\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "    \n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        return torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "        return self.act(x)\n",
    "    \n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = None\n",
    "        if time_emb_dim is not None:\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_emb_dim, dim_out * 2)\n",
    "            )\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if self.mlp is not None and time_emb is not None:\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "    \n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, dim, 1),\n",
    "            LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads), qkv)\n",
    "        q = q.softmax(dim=-2) * self.scale\n",
    "        k = k.softmax(dim=-1)\n",
    "        v = v / (h * w)\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        resnet_block_groups=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.init_conv = nn.Conv2d(channels, dim, 7, padding=3)\n",
    "        dims = [dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        block_cls = partial(ResnetBlock, groups=resnet_block_groups, time_emb_dim=time_dim)\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                block_cls(dim_in, dim_in),\n",
    "                block_cls(dim_in, dim_in),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding=1)\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_cls(mid_dim, mid_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, LinearAttention(mid_dim)))\n",
    "        self.mid_block2 = block_cls(mid_dim, mid_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                block_cls(dim_out + dim_in, dim_out),\n",
    "                block_cls(dim_out + dim_in, dim_out),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                Upsample(dim_out, dim_in) if not is_last else nn.Conv2d(dim_out, dim_in, 3, padding=1)\n",
    "            ]))\n",
    "\n",
    "        self.final_res_block = block_cls(dim * 2, dim)\n",
    "        self.final_conv = nn.Conv2d(dim, channels, 1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        x = self.init_conv(x)\n",
    "        residual = x.clone()\n",
    "        t = self.time_mlp(time)\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, down in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            x = down(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, up in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            x = up(x)\n",
    "\n",
    "        x = torch.cat((x, residual), dim=1)\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n",
    "    \n",
    "model = Unet(dim=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8543e9",
   "metadata": {},
   "source": [
    "### Diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589cf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        image_size: int, \n",
    "        timesteps: int,\n",
    "        beta_schedule: str = 'linear',\n",
    "        auto_normalize: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert model.channels == model.out_dim\n",
    "        assert not model.random_or_learned_sinusoidal_cond\n",
    "\n",
    "        self.model = model\n",
    "        self.channels = model.channels\n",
    "        self.image_size = image_size\n",
    "        self.num_timesteps = timesteps\n",
    "\n",
    "        # 1) build beta schedule\n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown beta schedule: {beta_schedule}')\n",
    "        \n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "        # 2) register all buffers\n",
    "        self.register_buffer('betas', betas.float())\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod.float())\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev.float())\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod - 1.0).float())\n",
    "\n",
    "        # posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance.float())\n",
    "        self.register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min=1e-20)).float())\n",
    "        self.register_buffer('posterior_mean_coef1', (betas * torch.sqrt(alphas_cumprod_prev) / (1 - alphas_cumprod)).float())\n",
    "        self.register_buffer('posterior_mean_coef2', ((1 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1 - alphas_cumprod)).float())\n",
    "\n",
    "        # 3) Normalization functions\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else (lambda x: x)\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else (lambda x: x)\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        \"\"\"Estimate x_0 from x_t and predicted noise\"\"\"\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t \n",
    "            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "    \n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        \"\"\" Compute mean & variance of q(x_{t-1}) | x_t, x_0 \"\"\"\n",
    "        mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        var = extract(self.posterior_variance, t, x_t.shape)\n",
    "        log_var = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return mean, var, log_var\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, clip_denoised: bool = True):\n",
    "        \"\"\" One diffusion reverse step \"\"\"\n",
    "        b = x.shape[0]\n",
    "        t_batch = torch.full((b,), t, device=x.device, dtype=torch.long)\n",
    "        # model predicts noise\n",
    "        pred_noise = self.model(x, t_batch)\n",
    "        # recover x0; optionally clamp it\n",
    "        x0 = self.predict_start_from_noise(x, t_batch, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x0 = x0.clamp(-1.0, 1.0)\n",
    "\n",
    "        # posterior mean & variance\n",
    "        mean, _, log_var = self.q_posterior(x0, x, t_batch)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0.0\n",
    "        return mean + torch.exp(0.5 * log_var) * noise\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps: bool = False):\n",
    "        \"\"\" Starting from pure noise, run full reverse chain\"\"\"\n",
    "        img = torch.randn(shape, device=self.betas.device)\n",
    "        all_imgs = [img]\n",
    "        for t in tqdm(reversed(range(self.num_timesteps)), desc='sampling'):\n",
    "            img = self.p_sample(img, t)\n",
    "            all_imgs.append(img)\n",
    "        out = torch.stack(all_imgs, dim=1) if return_all_timesteps else img\n",
    "        return self.unnormalize(out)\n",
    "    \n",
    "    def sample(self, batch_size: int = 16, return_all_timesteps: bool = False):\n",
    "        shape = (batch_size, self.channels, self.image_size, self.image_size)\n",
    "        return self.p_sample_loop(shape, return_all_timesteps)\n",
    "    \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\" Forward noising process \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start + \n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "    \n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        \"\"\" MSE loss between true noise and model's prediction \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        pred_noise = self.model(x_noisy, t)\n",
    "        loss = F.mse_loss(pred_noise, noise, reduction='none')\n",
    "        loss = loss.mean(dim=list(range(1, loss.ndim))) # mean over c, h, w\n",
    "        return loss.mean()\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \"\"\" Training entrypoint: sample random timesteps & return loss \"\"\"\n",
    "        b, c, h, w = *img.shape, img.device\n",
    "        assert h == self.image_size and w == self.image_size\n",
    "        t = torch.randint(0, self.num_timesteps, (b, ), device=img.device)\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798df8c3",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a30ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model: nn.Module,\n",
    "        data_folder: str,\n",
    "        batch_size: int = 16,\n",
    "        lr: float = 1e-4,\n",
    "        num_steps: int = 100000,\n",
    "        grad_accum_steps: int = 1,\n",
    "        ema_decay: float = 0.995,\n",
    "        save_interval: int = 1000,\n",
    "        num_samples: int = 25,\n",
    "        results_folder: str = './results'\n",
    "    ):\n",
    "        # Accelerator\n",
    "        self.accelerator = Accelerator(mixed_precision='no')\n",
    "        self.device = self.accelerator.device\n",
    "\n",
    "        # model, optimizer, EMA\n",
    "        self.model = diffusion_model.to(self.device)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "        self.model, self.optimizer = self.accelerator.prepare(\n",
    "            self.model, self.optimizer\n",
    "        )\n",
    "        \n",
    "        # Use EMA on the raw model\n",
    "        self.ema = EMA(self.acclerator.unwrap_model(self.model), beta=ema_decay)\n",
    "\n",
    "        # Data\n",
    "        ds = Dataset(data_folder, diffusion_model.image_size)\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count)\n",
    "        self.dl = cycle(self.accelerator.prepare(dl))\n",
    "\n",
    "        # Training State\n",
    "        self.batch_size       = batch_size\n",
    "        self.grad_accum_steps = grad_accum_steps\n",
    "        self.num_steps        = num_steps\n",
    "        self.save_interval    = save_interval\n",
    "        self.num_samples      = num_samples\n",
    "\n",
    "        # checkpoints & samples\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(parents=True, exist_ok=True)\n",
    "        self.step = 0\n",
    "    \n",
    "    def save(self, milestone: int):\n",
    "        \"\"\"Save model, optimizer, EMA and step counter.\"\"\"\n",
    "        if not self.accelerator.is_main_process:\n",
    "            return\n",
    "        ckpt = self.results_folder / f'model-{milestone}.pt'\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'ema':       self.ema.state_dict(),\n",
    "        }\n",
    "        torch.save(data, ckpt)\n",
    "\n",
    "    def load(self, ckpt_path: str):\n",
    "        \"\"\"Load all state (model, optimizer, EMA, step).\"\"\"\n",
    "        data = torch.load(ckpt_path, map_location=self.device)\n",
    "        raw_model = self.accelerator.unwrap_model(self.model)\n",
    "        raw_model.load_state_dict(data['model'])\n",
    "        self.optimizer.load_state_dict(data['optimizer'])\n",
    "        self.ema.load_state_dict(data['ema'])\n",
    "        self.step = data['step']\n",
    "\n",
    "    def _sample_and_save(self, milestone: int):\n",
    "        \"\"\"Generate `num_samples` via EMA model and save grid.\"\"\"\n",
    "        self.ema.ema_model.eval()\n",
    "        batches = num_to_groups(self.num_samples, self.batch_size)\n",
    "        imgs = torch.cat([\n",
    "            self.ema.ema_model.sample(batch_size=n) for n in batches\n",
    "        ], dim=0)\n",
    "        path = self.results_folder / f'sample-{milestone}.png'\n",
    "        vutils.save_image(imgs, path, nrow=int(math.sqrt(self.num_samples)))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Run the training loop with gradient accumulation, EMA updates, and periodic sampling.\"\"\"\n",
    "        pbar = tqdm(total=self.num_steps, initial=self.step, disable=not self.accelerator.is_main_process)\n",
    "        while self.step < self.num_steps:\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # gradient accumulation\n",
    "            for _ in range(self.grad_accum_steps):\n",
    "                batch = next(self.dl).to(self.device)\n",
    "                with self.accelerator.autocast():\n",
    "                    loss = self.model(batch) / self.grad_accum_steps\n",
    "                total_loss += loss.item()\n",
    "                self.accelerator.backward(loss)\n",
    "\n",
    "            # optimizer step\n",
    "            self.accelerator.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.step += 1\n",
    "\n",
    "            # EMA & sampling\n",
    "            if self.accelerator.is_main_process:\n",
    "                self.ema.update()\n",
    "                if self.step % self.save_interval == 0:\n",
    "                    milestone = self.step // self.save_interval\n",
    "                    self._sample_and_save(milestone)\n",
    "                    self.save(milestone)\n",
    "\n",
    "            pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "            pbar.update(1)\n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            print('Training complete.')\n",
    "\n",
    "    def inference(self, total: int = 1000, per_batch: int = 200, output_path: str = './submission'):\n",
    "        \"\"\"Generate `total` images in chunks of `per_batch` and write to disk.\"\"\"\n",
    "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            while count < total:\n",
    "                n = min(per_batch, total - count)\n",
    "                imgs = self.ema.ema_model.sample(batch_size=n)\n",
    "                for img in imgs:\n",
    "                    count += 1\n",
    "                    vutils.save_image(img, f'{output_path}/{count}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e7a7a",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e1a6b00",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Unet' object has no attribute 'out_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     13\u001b[0m beta_schedule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m Unet(\n\u001b[1;32m     16\u001b[0m     dim \u001b[38;5;241m=\u001b[39m channels,\n\u001b[1;32m     17\u001b[0m     dim_mults \u001b[38;5;241m=\u001b[39m dim_mults\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m diffusion \u001b[38;5;241m=\u001b[39m \u001b[43mGaussianDiffusion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_schedule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta_schedule\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     28\u001b[0m     diffusion,\n\u001b[1;32m     29\u001b[0m     data_folder \u001b[38;5;241m=\u001b[39m train_data_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     save_interval \u001b[38;5;241m=\u001b[39m timesteps\n\u001b[1;32m     36\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mGaussianDiffusion.__init__\u001b[0;34m(self, model, image_size, timesteps, beta_schedule, auto_normalize)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      4\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     auto_normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m ):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m model\u001b[38;5;241m.\u001b[39mchannels \u001b[38;5;241m==\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_dim\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mrandom_or_learned_sinusoidal_cond\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Unet' object has no attribute 'out_dim'"
     ]
    }
   ],
   "source": [
    "train_data_path = './face_dataset'\n",
    "IMG_SIZE = 64\n",
    "batch_size = 16\n",
    "train_num_steps = 10000\n",
    "lr = 1e-4\n",
    "grad_steps = 2\n",
    "ema_decay = 0.995\n",
    "\n",
    "channels = 16\n",
    "dim_mults = (1, 2, 4)\n",
    "\n",
    "timesteps = 1000\n",
    "beta_schedule = 'linear'\n",
    "\n",
    "model = Unet(\n",
    "    dim = channels,\n",
    "    dim_mults = dim_mults\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = IMG_SIZE,\n",
    "    timesteps = timesteps,\n",
    "    beta_schedule = beta_schedule\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    data_folder = train_data_path,\n",
    "    batch_size = batch_size,\n",
    "    lr = lr,\n",
    "    num_steps = train_num_steps,\n",
    "    grad_accum_steps = grad_steps,\n",
    "    ema_decay = ema_decay,\n",
    "    save_interval = timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae7de3",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = '/kaggle/working/results/model-10.pt'\n",
    "trainer.load(ckpt)\n",
    "trainer.inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
